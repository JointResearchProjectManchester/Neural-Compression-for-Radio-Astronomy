# Neural Compression for Radio Astronomy

## Overview
This repository contains the implementation of **Vector Quantized Variational Autoencoder (VQ-VAE)** and other neural compression techniques applied to radio astronomy images. The project is based on research conducted at the **University of Manchester, Department of Physics and Astronomy**, investigating the efficiency of neural compression models in handling the vast amount of data generated by modern radio telescopes.

## Project Motivation
Radio astronomy produces massive datasets, with instruments like the Square Kilometre Array (SKA) expected to generate up to 157 terabytes of raw data per second. Traditional compression methods are often insufficient, necessitating machine learning-based approaches such as **neural compression models**. This project compares the **VQ-VAE** against standard **Variational Autoencoders (VAE)** to evaluate their compression efficiency and ability to retain relevant astrophysical features.

## Features
- **VQ-VAE Implementation**: A neural compression model utilizing vector quantization in latent space.
- **Comparative Study**: Evaluates **VQ-VAE** against **VAE** in terms of compression efficiency.
- **Radio Galaxy Image Processing**: Works with datasets such as **MiraBest** and **Radio Galaxy Zoo**.
- **Loss Functions & Optimization**: Implements a combination of **reconstruction loss**, **codebook loss**, and **commitment loss**.
- **Noise Handling & Image Stacking**: Examines the model's effectiveness in denoising stacked images.
- **Latent Space Visualization**: Uses **Uniform Manifold Approximation and Projection (UMAP)** to visualize learned feature representations.

## Datasets Used
- **CIFAR-10**: Used initially for debugging and model validation.
- **MiraBest Dataset**: A set of 1256 grayscale radio galaxy images (150x150 pixels).
- **Radio Galaxy Zoo Dataset**: Nearly **120,000** radio galaxy images, used for model training and evaluation.

## Model Architecture
The **VQ-VAE** consists of the following key components:
- **Encoder**: Converts high-dimensional input into a lower-dimensional latent representation.
- **Quantizer**: Maps continuous latent variables to discrete codebook embeddings.
- **Decoder**: Reconstructs the original data from the quantized latent representation.
- **Loss Function**:
  - **Reconstruction Loss**: Measures the difference between original and reconstructed images.
  - **Codebook Loss**: Optimizes embedding vectors for better quantization.
  - **Commitment Loss**: Prevents embeddings from growing arbitrarily large.

## Results
- **Compression Performance**:
  - VQ-VAE achieved a **bits-per-dimension (BPD) of 2.68**, compared to **3.89** for the standard VAE, demonstrating better compression efficiency.
- **Image Stacking & Noise Handling**:
  - The VQ-VAE struggled to retain faint astrophysical signals when applied to noisy image stacks, suggesting potential improvements in latent space learning.
- **Latent Space Representation**:
  - UMAP visualizations showed distinct feature clusters, demonstrating the model's ability to learn meaningful astrophysical features.

## Installation & Usage
### Prerequisites
Ensure you have **Python 3.x** and the required libraries installed:
```bash
pip install torch torchvision numpy matplotlib umap-learn
```

### Running the Model
1. Clone the repository:
```bash
git clone https://github.com/Eazo1/Neural_Compression_Masters.git
cd Neural_Compression_Masters
```
2. Train the model using the dataset:
```bash
python main_VQVAE.py


## Future Work
- **Diffusion Denoising Model**
- **Improve Noise Robustness**: Address VQ-VAEâ€™s limitations in retaining low-signal information.
- **Hyperparameter Optimization**: Explore different learning rates, latent space sizes, and architectures.
- **Alternative Compression Models**: Investigate denoising diffusion models for enhanced feature retention.
- **Astronomical Applications**: Test model deployment for real-time telescope data processing.

## License
This project is licensed under the **MIT License**. See [LICENSE](LICENSE) for more details.

